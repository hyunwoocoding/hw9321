{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 코드와 다르니 다시 한 번 확인해볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논리게이트 - AND, OR, NAND, XOR\n",
    "\n",
    "##  AND, OR, NAND, XOR 논리테이블(Logic Table)은 입력데이터(x1, x2), 정답데이터 t (0 또는 1)인 머신러닝 training data와 개념적으로 동일함\n",
    "\n",
    "## 즉, 논리게이트는 손실함수로 cross-entropy를 이용해서 Logistic Regression(Classification) 알고리즘으로 데이터를 분류하고 결과를 예측할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external function (sigmoid, numerical_derivative)\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def numerical_derivative(f, x): #x는 모든 변수를 포함하고 있는 numpy객체(배열, 행렬...)\n",
    "    delta_x = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags=['readwrite']) #모든 입력변수에 대해 편미분하기 위해 iterator 사용\n",
    "    \n",
    "    while not it.finished: \n",
    "        idx = it.multi_index\n",
    "        \n",
    "        tmp_val = x[idx] \n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) #f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logicgate class\n",
    "\n",
    "class LogicGate:\n",
    "    \n",
    "    def __init__(self, gate_name, xdata, tdata): #xdata, tdata => numpy.array(...)\n",
    "        \n",
    "        self.name = gate_name\n",
    "        \n",
    "        #입력 데이터, 정답 데이터 초기화\n",
    "        self.__xdata = xdata.reshape(4, 2) # 입력데이터는 (0, 0), (0, 1), (1, 0), (1, 1) 총 4가지\n",
    "        self.__tdata = tdata.reshape(4, 1)\n",
    "        \n",
    "        #가중치 W, 바이어스 b 초기화\n",
    "        self.__W = np.random.rand(2, 1) #weight, 2x1 matrix\n",
    "        self.__b = np.random.rand(1)\n",
    "        \n",
    "        #가중치w와 바이어스b는 모두 priavate으로 선언되어 있다는 것\n",
    "        #왜냐하면 파이선 클래스의 variable은 기본적으로  public이기 때문에 외부에서 바꿀 가능성이 있음\n",
    "        #하지만 w, b는 외부에서 바꾸면 안되기 때문에 내부 method만을 통해서 바꿀 수 있게 안전장치 설치\n",
    "        \n",
    "        #학습률 learning rate 초기화\n",
    "        self.__learning_rate = 1e-2 #발산하는 경우 더 작게 (1e-3, 1e-4...)\n",
    "\n",
    "    #손실함수\n",
    "    def __loss_func(self):\n",
    "\n",
    "        delta = 1e-7 # log 무한대 발산 방지 / y가 0이나 1인 경우 무한대가 되어 버리기 때문에 이를 방지\n",
    "\n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "\n",
    "        #cross-entropy\n",
    "\n",
    "        return -np.sum(self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y) + delta))\n",
    "    \n",
    "    #손실 값 계산\n",
    "    \n",
    "    def error_val(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "\n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "\n",
    "        #cross-entropy\n",
    "        return -np.sum(self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y) + delta))\n",
    "    #수치미분을 이용하여 손실함수가 최소가 될 때까지 학습하는 함수\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        f = lambda x : self.__loss_func()\n",
    "\n",
    "        print(\"initial error value =\", self.error_val())\n",
    "\n",
    "        for step in range(8001):\n",
    "            self.__W -= self.__learning_rate + numerical_derivative(f, self.__W)\n",
    "            self.__b -= self.__learning_rate + numerical_derivative(f, self.__b)\n",
    "\n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val())\n",
    "\n",
    "    # 미래 값 예측 함수\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        z = np.dot(input_data, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "\n",
    "        if y > 0.5:\n",
    "            result = 1 # True\n",
    "        else:\n",
    "            result = 0 # False\n",
    "\n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value = 3.6364490485663126\n",
      "step =  0 error value =  2.3160981658765643\n",
      "step =  400 error value =  0.04645309202166194\n",
      "step =  800 error value =  0.027473141903401874\n",
      "step =  1200 error value =  0.021212742186327783\n",
      "step =  1600 error value =  0.01816038384385633\n",
      "step =  2000 error value =  0.016375719182666382\n",
      "step =  2400 error value =  0.015213865840320268\n",
      "step =  2800 error value =  0.014401346480405931\n",
      "step =  3200 error value =  0.013803251851151607\n",
      "step =  3600 error value =  0.013345702506689303\n",
      "step =  4000 error value =  0.012985002459024632\n",
      "step =  4400 error value =  0.012693728420059384\n",
      "step =  4800 error value =  0.01245383800762601\n",
      "step =  5200 error value =  0.012252994949942529\n",
      "step =  5600 error value =  0.012082490090875572\n",
      "step =  6000 error value =  0.011936006348985777\n",
      "step =  6400 error value =  0.011808854315935879\n",
      "step =  6800 error value =  0.011697482310930263\n",
      "step =  7200 error value =  0.011599152727559752\n",
      "step =  7600 error value =  0.01151172252769418\n",
      "step =  8000 error value =  0.011433490878188629\n"
     ]
    }
   ],
   "source": [
    "#입력 데이터 / 정답 데이터\n",
    "\n",
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 0, 0, 1]) #AND 정답데이터\n",
    "\n",
    "AND_obj = LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "\n",
    "AND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AND Gate prediction\n",
    "print(AND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value = 1.9168827921827731\n",
      "step =  0 error value =  1.4460862724242567\n",
      "step =  400 error value =  0.041941213816685606\n",
      "step =  800 error value =  0.03309941765414632\n",
      "step =  1200 error value =  0.03107352376720691\n",
      "step =  1600 error value =  0.03045688073026242\n",
      "step =  2000 error value =  0.030253360185853045\n",
      "step =  2400 error value =  0.03018439955827074\n",
      "step =  2800 error value =  0.030160824970061128\n",
      "step =  3200 error value =  0.030152741441291375\n",
      "step =  3600 error value =  0.030149966791482335\n",
      "step =  4000 error value =  0.030149014061534987\n",
      "step =  4400 error value =  0.030148686882342974\n",
      "step =  4800 error value =  0.03014857452158534\n",
      "step =  5200 error value =  0.030148535933341828\n",
      "step =  5600 error value =  0.030148522680856764\n",
      "step =  6000 error value =  0.030148518129443913\n",
      "step =  6400 error value =  0.03014851656641042\n",
      "step =  6800 error value =  0.03014851602951671\n",
      "step =  7200 error value =  0.030148515845269203\n",
      "step =  7600 error value =  0.030148515781926073\n",
      "step =  8000 error value =  0.03014851576057541\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "OR_obj = LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "\n",
    "OR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR Gate prediction\n",
    "print(OR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = OR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value = 2.7621611123019223\n",
      "step =  0 error value =  2.077007963148196\n",
      "step =  400 error value =  0.036879191135763864\n",
      "step =  800 error value =  0.019373291491657268\n",
      "step =  1200 error value =  0.015025561408556324\n",
      "step =  1600 error value =  0.013801501880631787\n",
      "step =  2000 error value =  0.01346795902970297\n",
      "step =  2400 error value =  0.013379403190639232\n",
      "step =  2800 error value =  0.013356111524461915\n",
      "step =  3200 error value =  0.01335000202582214\n",
      "step =  3600 error value =  0.013348400649521202\n",
      "step =  4000 error value =  0.013347980989402349\n",
      "step =  4400 error value =  0.013347871017996484\n",
      "step =  4800 error value =  0.013347842200663783\n",
      "step =  5200 error value =  0.01334783464925277\n",
      "step =  5600 error value =  0.01334783267023341\n",
      "step =  6000 error value =  0.013347832151487849\n",
      "step =  6400 error value =  0.013347832015648113\n",
      "step =  6800 error value =  0.013347831980087689\n",
      "step =  7200 error value =  0.013347831970664806\n",
      "step =  7600 error value =  0.013347831967957647\n",
      "step =  8000 error value =  0.013347831967485634\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([1, 1, 1, 0])\n",
    "\n",
    "NAND_obj = LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "\n",
    "NAND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate prediction\n",
    "print(NAND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value = 3.529670423947959\n",
      "step =  0 error value =  2.7793188961226716\n",
      "step =  400 error value =  2.7727379354256856\n",
      "step =  800 error value =  2.7727379354256994\n",
      "step =  1200 error value =  2.772737935425646\n",
      "step =  1600 error value =  2.772737935425614\n",
      "step =  2000 error value =  2.7727379354256056\n",
      "step =  2400 error value =  2.772737935425619\n",
      "step =  2800 error value =  2.772737935425609\n",
      "step =  3200 error value =  2.7727379354256447\n",
      "step =  3600 error value =  2.772737935425592\n",
      "step =  4000 error value =  2.772737935425627\n",
      "step =  4400 error value =  2.772737935425596\n",
      "step =  4800 error value =  2.772737935425698\n",
      "step =  5200 error value =  2.772737935425556\n",
      "step =  5600 error value =  2.772737935425613\n",
      "step =  6000 error value =  2.7727379354255604\n",
      "step =  6400 error value =  2.772737935425618\n",
      "step =  6800 error value =  2.7727379354256088\n",
      "step =  7200 error value =  2.7727379354256443\n",
      "step =  7600 error value =  2.7727379354256354\n",
      "step =  8000 error value =  2.7727379354255817\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 0]) #XOR 정답 데이터\n",
    "\n",
    "\n",
    "XOR_obj = LogicGate(\"XOR_GATE\", xdata, tdata)\n",
    "\n",
    "# XOR Gate 를 보면, 손실함수 값이 2.7 근처에서 더 이상 감소하지 않는것을 볼수 있음\n",
    "XOR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR Gate prediction => 예측이 되지 않음\n",
    "print(XOR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = XOR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XOR 을 NAND + OR => AND 조합으로 계산함\n",
    "input_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "s1 = []    # NAND 출력\n",
    "s2 = []    # OR 출력\n",
    "\n",
    "new_input_data = []  # AND 입력\n",
    "final_output = []    # AND 출력\n",
    "\n",
    "for index in range(len(input_data)):\n",
    "    \n",
    "    s1 = NAND_obj.predict(input_data[index])  # NAND 출력\n",
    "    s2 = OR_obj.predict(input_data[index])    # OR 출력\n",
    "    \n",
    "    new_input_data.append(s1[-1])    # AND 입력\n",
    "    new_input_data.append(s2[-1])    # AND 입력\n",
    "    \n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(np.array(new_input_data))\n",
    "    \n",
    "    final_output.append(logical_val)    # AND 출력, 즉 XOR 출력    \n",
    "    new_input_data = []    # AND 입력 초기화\n",
    "\n",
    "\n",
    "for index in range(len(input_data)):    \n",
    "    print(input_data[index], \" = \", final_output[index], end='')\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
